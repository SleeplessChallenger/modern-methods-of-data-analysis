{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b14de96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "ind_claster = {0: [2, 5, 7, 8, 17, 18, 19, 24, 35, 50, 58, 62, 92,\n",
    "                   94, 129, 134, 154, 171, 174, 205, 227, 331, 336, 349, 416, 424,\n",
    "                   468, 484],\n",
    "               1: [10, 11, 13, 14, 16, 20, 23, 28, 33, 34, 39, 45, 46,\n",
    "                   47, 53, 59, 61, 63, 64, 79, 85, 86, 88, 89, 97, 100,\n",
    "                   106, 109, 116, 124, 126, 128, 137, 139, 146, 151, 153, 161, 165,\n",
    "                   168, 176, 178, 179, 182, 183, 196, 210, 228, 231, 232, 233, 244,\n",
    "                   253, 257, 269, 272, 293, 308, 309, 318, 330, 338, 340, 358, 363,\n",
    "                   374, 383, 390, 403, 407, 411, 412, 413, 432, 434, 439, 442, 449,\n",
    "                   463, 469, 471, 476, 488, 496],\n",
    "               2: [0, 1, 15, 21, 25, 29, 31, 32, 37, 43, 44, 48, 51,\n",
    "                   55, 56, 60, 65, 67, 68, 70, 73, 74, 75, 80, 82, 95,\n",
    "                   104, 125, 132, 133, 136, 138, 141, 142, 143, 145, 163, 167, 169,\n",
    "                   172, 175, 181, 187, 188, 189, 194, 197, 204, 211, 213, 215, 219,\n",
    "                   221, 223, 224, 238, 240, 256, 260, 262, 263, 264, 267, 268, 271,\n",
    "                   275, 277, 278, 279, 284, 286, 288, 296, 297, 300, 305, 306, 307,\n",
    "                   314, 317, 323, 324, 327, 328, 329, 332, 347, 351, 354, 359, 360,\n",
    "                   364, 365, 375, 381, 387, 388, 394, 401, 409, 417, 418, 419, 421,\n",
    "                   422, 427, 428, 431, 436, 437, 438, 441, 443, 446, 447, 450, 453,\n",
    "                   459, 460, 462, 464, 478, 482, 489, 491, 497, 498, 499],\n",
    "               3: [4, 6, 9, 12, 22, 26, 27, 30, 36, 38, 40, 41, 42,\n",
    "                   49, 52, 54, 57, 66, 69, 71, 72, 76, 77, 78, 81, 84,\n",
    "                   87, 90, 91, 93, 96, 98, 99, 101, 102, 103, 105, 107, 108,\n",
    "                   110, 111, 112, 113, 114, 115, 117, 118, 119, 120, 121, 122, 123,\n",
    "                   127, 130, 131, 135, 140, 144, 147, 148, 149, 150, 152, 155, 156,\n",
    "                   157, 158, 159, 160, 162, 164, 166, 170, 173, 177, 180, 184, 185,\n",
    "                   186, 190, 191, 192, 193, 195, 198, 199, 200, 201, 202, 203, 206,\n",
    "                   207, 208, 209, 212, 214, 217, 218, 220, 222, 225, 226, 229, 230,\n",
    "                   234, 235, 236, 237, 239, 241, 242, 243, 245, 246, 247, 248, 249,\n",
    "                   250, 251, 252, 254, 255, 258, 259, 261, 265, 266, 270, 273, 274,\n",
    "                   276, 280, 281, 282, 283, 285, 287, 289, 290, 291, 292, 294, 295,\n",
    "                   298, 299, 301, 302, 303, 304, 310, 311, 312, 313, 315, 316, 319,\n",
    "                   320, 321, 322, 325, 326, 333, 334, 335, 337, 339, 341, 342, 343,\n",
    "                   344, 345, 346, 348, 350, 352, 353, 355, 356, 357, 361, 362, 366,\n",
    "                   367, 368, 369, 370, 371, 372, 373, 376, 377, 378, 379, 380, 382,\n",
    "                   384, 385, 386, 389, 391, 392, 393, 395, 396, 397, 398, 399, 400,\n",
    "                   402, 404, 405, 406, 408, 410, 414, 415, 420, 425, 426, 429, 430,\n",
    "                   433, 435, 440, 444, 445, 448, 451, 452, 454, 455, 456, 457, 458,\n",
    "                   461, 465, 466, 467, 470, 472, 474, 475, 477, 479, 480, 481, 483,\n",
    "                   485, 486, 487, 490, 492, 493, 494, 495],\n",
    "               4: [3, 83, 216, 423, 473]}\n",
    "\n",
    "x = pd.read_excel()\n",
    "x.corr()\n",
    "\n",
    "x['cl'] = 0\n",
    "for i in range(5):\n",
    "    x.iloc[ind_claster[i], -1] = i\n",
    "\n",
    "# because of the high correlations we take that 3 features\n",
    "x_selected = np.array(x[['InboundCalls', 'MonthlyMinutes', 'OutboundCalls']])\n",
    "\n",
    "\n",
    "def standardizer(x):\n",
    "    \"\"\"\n",
    "    standardize entity-to-feature data matrix by\n",
    "    applying Z-scoring, Range standardization and Rank methods\n",
    "    Arguments:\n",
    "        x, numpy array, entity-to-feature data matrix\n",
    "        Returns:\n",
    "        Z-\n",
    "    scored and Range standardized data matrices, x_zscore, x_range, x_rank.\n",
    "    \"\"\"\n",
    "    x_ave = np.mean(x, axis=0)\n",
    "    x_min = np.min(x, axis=0)\n",
    "    x_rng = np.ptp(x, axis=0)\n",
    "    x_std = np.std(x, axis=0)\n",
    "    x_zscore = np.divide(np.subtract(x, x_ave), x_std)  # # Z-scoring standardization\n",
    "    x_range = np.divide(np.subtract(x, x_ave),\n",
    "                        x_rng)  # Range standardization\n",
    "    x_rank = np.divide(np.subtract(x, x_min), x_rng)\n",
    "    # Rank standardization\n",
    "    return x_zscore, x_range, x_rank\n",
    "\n",
    "\n",
    "def singular_decomposition(x):\n",
    "    z, mu, c = np.linalg.svd(x, full_matrices=True)\n",
    "    z = -z\n",
    "    c = -c\n",
    "    mu_arg_max = np.argmax(mu)\n",
    "    # print(\"Shape: \", z.shape, mu.shape, c.shape)\n",
    "    # print(\"mu:\", mu)\n",
    "    # print(\"loadings:\", )\n",
    "    # print(c)\n",
    "    # print(\" \")\n",
    "    n_contributions = np.power(mu, 2)\n",
    "    ds = np.sum(np.power(x, 2))  # Data Scatter\n",
    "\n",
    "    p_contributions = np.divide(n_contributions, ds)\n",
    "\n",
    "    print(\"natural contributions:\", n_contributions)\n",
    "    print(\"percent contributions:\", 100 * p_contributions)\n",
    "    return z, mu, c, ds, n_contributions, p_contributions, mu_arg_max\n",
    "\n",
    "\n",
    "x_zscore, x_range, x_rank = standardizer(x=x_selected)\n",
    "z_x, mu_x, c_x, ds_x, n_contributions_x, p_contributions_x, mu_argmax_x = singular_decomposition(x=x_selected)\n",
    "\n",
    "# Singular decomposition on Z-Scored standardized data\n",
    "z_x_zscore, mu_x_zscore, c_x_zscore, ds_x_zscore, n_contributions_x_zscore, p_contributions_x_zscore, mu_argmax_x_zscore = singular_decomposition(\n",
    "    x=x_zscore)\n",
    "\n",
    "# Singular decomposition on Range standardized data\n",
    "z_x_range, mu_x_range, c_x_range, ds_x_range, n_contributions_x_range, p_contributions_x_range, mu_argmax_x_range = singular_decomposition(\n",
    "    x=x_range)\n",
    "\n",
    "# Singular decomposition on Rank standardized data\n",
    "z_x_rank, mu_x_rank, c_x_rank, ds_x_rank, \\\n",
    "    n_contributions_x_rank, p_contributions_x_rank, mu_argmax_x_rank = \\\n",
    "    singular_decomposition(x=x_rank)\n",
    "\n",
    "print(\"Natural contributions of \\n\",\n",
    "      \"Original data 131. : \", n_contributions_x,\n",
    "      \"Ranged std data: \", n_contributions_x_range,\n",
    "      \"Z-Scored std data: \", n_contributions_x_zscore)\n",
    "\n",
    "\"\"\"\n",
    "Ranged standardized data [82.91 10.81 6.27 ]: :\n",
    "First component contribution is equal to 82.91%\n",
    "Second component contribution is equal to 10.81%\n",
    "\"\"\"\n",
    "\n",
    "# Considering Z-Score standardized data: c1 = [0.553 0.577 0.601]\n",
    "# interpretation: all loadings are positive and more or less equal\n",
    "# to each other.\n",
    "\n",
    "z_x_zscore.shape\n",
    "cl_4_ind = np.where(x.iloc[:, -1] == 4)[0]\n",
    "cl_3_ind = np.where(x.iloc[:, -1] == 3)[0]\n",
    "cl_2_ind = np.where(x.iloc[:, -1] == 2)[0]\n",
    "cl_1_ind = np.where(x.iloc[:, -1] == 1)[0]\n",
    "cl_0_ind = np.where(x.iloc[:, -1] == 0)[0]\n",
    "\n",
    "z_z0 = z_x_zscore[:, 0] * np.sqrt(mu_x_zscore[0])\n",
    "z_z1 = z_x_zscore[:, 1] * np.sqrt(mu_x_zscore[1])\n",
    "\n",
    "z_r0 = z_x_range[:, 0] * np.sqrt(mu_x_range[0])\n",
    "z_r1 = z_x_range[:, 1] * np.sqrt(mu_x_range[1])\n",
    "fig = plt.figure(figsize=(20, 10))\n",
    "\n",
    "ax = fig.add_subplot(121)\n",
    "ax.scatter(z_z0, z_z1)\n",
    "ax.scatter(z_z0[cl_4_ind], z_z1[cl_4_ind], c='black', )\n",
    "ax.scatter(z_z0[cl_3_ind], z_z1[cl_3_ind], c='blue', )\n",
    "ax.scatter(z_z0[cl_2_ind], z_z1[cl_2_ind], c='green', )\n",
    "ax.scatter(z_z0[cl_1_ind], z_z1[cl_1_ind], c='yellow', )\n",
    "ax.scatter(z_z0[cl_0_ind], z_z1[cl_0_ind], c='red', )\n",
    "plt.title(\"PCA of Z-scoring standardized data\")\n",
    "\n",
    "ax = fig.add_subplot(122)\n",
    "ax.scatter(z_r0, z_r1)\n",
    "ax.scatter(z_r0[cl_4_ind], z_r1[cl_4_ind], c='black', )\n",
    "ax.scatter(z_r0[cl_3_ind], z_r1[cl_3_ind], c='blue', )\n",
    "\n",
    "ax.scatter(z_r0[cl_2_ind], z_r1[cl_2_ind], c='green', )\n",
    "ax.scatter(z_r0[cl_1_ind], z_r1[cl_1_ind], c='yellow', )\n",
    "ax.scatter(z_r0[cl_0_ind], z_r1[cl_0_ind], c='red', )\n",
    "plt.title(\"PCA of Range standardatized data\")\n",
    "plt.show()\n",
    "\n",
    "c_x_zscore[:3, :]\n",
    "\n",
    "\n",
    "def conventional_pca(X, standardized=False):\n",
    "    # Let us name x as original data set,\n",
    "    # restricted at selected subset of features, i.e. x_selected.\n",
    "    if standardized is False:\n",
    "        mean_x = np.mean(X, axis=0)\n",
    "        Y = np.subtract(X, mean_x)  # centered version\n",
    "        B = (Y.T @ Y) / Y.shape[0]  # covariance matrix of Y\n",
    "        L, C = np.linalg.eig(B)  # Eigenvalues\n",
    "        sorted_idx = np.argsort(L)[::-1]  # descending order\n",
    "        la1 = L[sorted_idx[0]]\n",
    "        c1 = -C[:, sorted_idx[0]]  # unlike np.linalg.svd now we should consider the column\n",
    "        pc1 = np.divide(Y @ c1, np.sqrt(Y.shape[0] * la1))  # 1st principle component\n",
    "        B_dot = B - la1 * np.multiply(c1, c1.T)  # Residual of Cov.\n",
    "        L_, C_ = np.linalg.eig(B_dot)\n",
    "        argmax_ = np.argmax(L_)\n",
    "        la2 = L_[argmax_]\n",
    "        c2 = -C_[:, argmax_]\n",
    "        pc2 = np.divide(Y @ c2, np.sqrt(Y.shape[0] * la2))  # 2nd principle component\n",
    "    else:\n",
    "        Y = X\n",
    "        B = (Y.T @ Y) / Y.shape[0]  # covariance matrix of Y\n",
    "        L, C = np.linalg.eig(B)\n",
    "        sorted_idx = np.argsort(L)[::-1]  # descending order\n",
    "        la1 = L[sorted_idx[0]]\n",
    "        c1 = C[:, sorted_idx[0]]  # unlike np.linalg.svd now we should consi\n",
    "        pc1 = np.divide(Y @ c1, np.sqrt(Y.shape[0] * la1))  # 1st principle comp\n",
    "        la2 = L[sorted_idx[1]]\n",
    "        c2 = -C[:, sorted_idx[1]]\n",
    "        pc2 = np.divide(Y @ c2, np.sqrt(Y.shape[0] * la2))\n",
    "\n",
    "    return pc1, pc2\n",
    "\n",
    "\n",
    "pc1_x, pc2_x = conventional_pca(X=x_selected, standardized=False)\n",
    "pc1_x_z, pc2_x_z = conventional_pca(X=x_zscore, standardized=True)\n",
    "pc1_x_range, pc2_x_range = conventional_pca(X=x_range, standardized=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff3e0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "pcas = pca.fit_transform(x_zscore)\n",
    "pc1, pc2 = pcas[:, 0], pcas[:, 1]\n",
    "pcas = pca.fit_transform(x_range)\n",
    "pc1_r, pc2_r = pcas[:, 0], pcas[:, 1]\n",
    "\n",
    "fig = plt.figure(figsize=(20, 10))\n",
    "ax = fig.add_subplot(151)\n",
    "# ax.scatter(z_z0, z_z1)\n",
    "ax.scatter(z_z0[cl_4_ind], z_z1[cl_4_ind], c='black', )\n",
    "ax.scatter(z_z0[cl_3_ind], z_z1[cl_3_ind], c='blue', )\n",
    "ax.scatter(z_z0[cl_2_ind], z_z1[cl_2_ind], c='green', )\n",
    "ax.scatter(z_z0[cl_1_ind], z_z1[cl_1_ind], c='yellow', )\n",
    "ax.scatter(z_z0[cl_0_ind], z_z1[cl_0_ind], c='red', )\n",
    "plt.title(\"PCA of Z-scoring data(SVD)\")\n",
    "\n",
    "ax = fig.add_subplot(152)\n",
    "# ax.scatter(pc1_x_z, pc2_x_z)\n",
    "ax.scatter(pc1_x_z[cl_4_ind], pc2_x_z[cl_4_ind], c='black', )\n",
    "ax.scatter(pc1_x_z[cl_3_ind], pc2_x_z[cl_3_ind], c='blue', )\n",
    "ax.scatter(pc1_x_z[cl_2_ind], pc2_x_z[cl_2_ind], c='green', )\n",
    "ax.scatter(pc1_x_z[cl_1_ind], pc2_x_z[cl_1_ind], c='yellow', )\n",
    "ax.scatter(pc1_x_z[cl_0_ind], pc2_x_z[cl_0_ind], c='red', )\n",
    "plt.title(\"Conventional PCA of Z-scored\")\n",
    "\n",
    "ax = fig.add_subplot(154)\n",
    "# ax.scatter(z_r0, z_r1)\n",
    "ax.scatter(z_r0[cl_4_ind], z_r1[cl_4_ind], c='black', )\n",
    "ax.scatter(z_r0[cl_3_ind], z_r1[cl_3_ind], c='blue', )\n",
    "ax.scatter(z_r0[cl_1_ind], z_r1[cl_1_ind], c='yellow', )\n",
    "ax.scatter(z_r0[cl_0_ind], z_r1[cl_0_ind], c='red', )\n",
    "plt.title(\"PCA of Range data(SVD)\")\n",
    "\n",
    "\n",
    "def part_3(x, z, c, mu, argmax):\n",
    "    z1 = z[:, argmax]  # hiden score\n",
    "    c1 = c[argmax, :]  # loading\n",
    "    mu1 = mu[argmax]\n",
    "    print(\"c1 :\", c1)\n",
    "    alpha = 1 / np.sum(c1)\n",
    "    loading = c1 * alpha\n",
    "    print('alpha :', alpha)\n",
    "    print(\"Loading :\", loading)\n",
    "    ranking_factors = 100 * x @ (loading)\n",
    "    arg_top5 = np.argsort(ranking_factors)[::-1][:4]\n",
    "    top_5 = np.sort(ranking_factors)[::-1][:4]\n",
    "    print(\"Top4 CreditRating in dataset:\", top_5)\n",
    "    return loading, ranking_factors, arg_top5\n",
    "\n",
    "\n",
    "_, ranking_factors, arg_top5 = part_3(x=x_rank, z=z_x_rank, c=c_x_rank, mu=mu_x_rank, argmax=mu_argmax_x_ra\n",
    "nk)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
